{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_4_ part_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xhU3R-8dzk5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
        "\n",
        "## RNNs, CNNs, AutoML, and more...\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
        "\n",
        "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
      ]
    },
    {
      "metadata": {
        "id": "-5UwGRnJOmD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - RNNs\n",
        "\n",
        "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
        "\n",
        "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
      ]
    },
    {
      "metadata": {
        "id": "lPn6c0x21gu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
      ]
    },
    {
      "metadata": {
        "id": "3if1yTMUoG3U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install twitterscraper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DS-9ksWjoJit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from twitterscraper import query_tweets\n",
        "\n",
        "austen_tweets = query_tweets('from:austen', 1000)\n",
        "len(austen_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLKqFh8DovaN",
        "colab_type": "code",
        "outputId": "dd6f120d-9567-4d3f-ad5e-32d300f6761b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "austen_tweets[0].text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "MRQeIIf1orCS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
        "len(al_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dB7I87ty8f1",
        "colab_type": "code",
        "outputId": "84a2446f-d81a-40d2-921c-1d579f14334d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "al_tweets[0].text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'RT @GeoffTheRobot: Hey Al, you played zydeco on my ribs at the RED premiere and it airs tonight on Late Late with @CraigyFerg!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "0mrcjEu_zRl4",
        "colab_type": "code",
        "outputId": "0e325d87-6652-47e0-b91b-db11bb8092ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "len(austen_tweets + al_tweets)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1141"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "WYCVJX6ep8iO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your tasks:\n",
        "\n",
        "- Encode the characters to a sequence of integers for the model\n",
        "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
        "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
        "\n",
        "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "metadata": {
        "id": "rLL5bnUadea8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XD8PHc4OaFZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Integer-encode all the characters in the corpus, and transform each\n",
        "# tweet into a list of integers (one per character).  Combine all tweets into a\n",
        "# numpy array that pads each tweet to the same length.\n",
        "\n",
        "def tweet_preprocessing(tweets):\n",
        "  '''\n",
        "  Pre-processes a corpus of tweets\n",
        "  '''\n",
        "  fulltext = ''\n",
        "  for twt in tweets:\n",
        "    fulltext += twt.text\n",
        "    \n",
        "  chars = list(set(fulltext)) # split and remove duplicate characters. convert to list.\n",
        "  num_chars = len(chars) # the number of unique characters\n",
        "  txt_data_size = len(fulltext)\n",
        "  \n",
        "  # Integer-encoded dictionaries\n",
        "  char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "  int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "  \n",
        "  # Turn each tweet into a list of integers representing the characters\n",
        "  encoded_tweets = []\n",
        "  for twt in tweets:\n",
        "    enc_tweet = [char_to_int[ch] for ch in twt.text]\n",
        "    encoded_tweets.append(enc_tweet)\n",
        "  \n",
        "  # Pad each tweet to 280 characters\n",
        "  padded_tweets = sequence.pad_sequences(encoded_tweets, maxlen=280)\n",
        "  \n",
        "#   # Informative printouts\n",
        "  print(\"Unique characters : \", num_chars)\n",
        "  print(\"Size of tweet library (char) : \", txt_data_size)\n",
        "  print('All characters: \\n', [x for x in chars])\n",
        "  print('Character dictionary: \\n', char_to_int)\n",
        "  print('Example processed tweet: \\n', padded_tweets[0])\n",
        "  print('Shape of tweet library: ', padded_tweets.shape)\n",
        "  \n",
        "  return padded_tweets, chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9613ylIwX3pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "b961aabc-9a91-48dc-9adf-7a73be37b46b"
      },
      "cell_type": "code",
      "source": [
        " austen_tweets2, austen_chars = tweet_preprocessing(austen_tweets)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters :  90\n",
            "Size of tweet library (char) :  16177\n",
            "All characters: \n",
            " ['X', 'k', 'T', ';', 'D', ')', 'f', 'h', '-', 'J', '…', 'W', 'F', 'a', '5', '\\n', '8', '1', 'I', 'K', 's', 'b', 'r', 'y', 'Q', 'A', '/', 'C', '’', 'c', 'Y', '?', 'e', ':', '(', 'u', '$', 'S', 'l', \"'\", '0', 'x', 'd', 'j', '“', '\"', 'M', '7', 'g', 'E', ' ', '3', 'p', 'H', 'm', '*', '2', 'O', '\\xa0', 'o', ',', 'i', 't', '@', '6', '4', 'ï', 'v', 'N', 'G', '#', 'U', '%', '!', 'n', 'q', 'z', 'B', 'w', '+', 'P', 'Z', 'R', '_', '.', 'V', '”', '&', '9', 'L']\n",
            "Character dictionary: \n",
            " {'X': 0, 'k': 1, 'T': 2, ';': 3, 'D': 4, ')': 5, 'f': 6, 'h': 7, '-': 8, 'J': 9, '…': 10, 'W': 11, 'F': 12, 'a': 13, '5': 14, '\\n': 15, '8': 16, '1': 17, 'I': 18, 'K': 19, 's': 20, 'b': 21, 'r': 22, 'y': 23, 'Q': 24, 'A': 25, '/': 26, 'C': 27, '’': 28, 'c': 29, 'Y': 30, '?': 31, 'e': 32, ':': 33, '(': 34, 'u': 35, '$': 36, 'S': 37, 'l': 38, \"'\": 39, '0': 40, 'x': 41, 'd': 42, 'j': 43, '“': 44, '\"': 45, 'M': 46, '7': 47, 'g': 48, 'E': 49, ' ': 50, '3': 51, 'p': 52, 'H': 53, 'm': 54, '*': 55, '2': 56, 'O': 57, '\\xa0': 58, 'o': 59, ',': 60, 'i': 61, 't': 62, '@': 63, '6': 64, '4': 65, 'ï': 66, 'v': 67, 'N': 68, 'G': 69, '#': 70, 'U': 71, '%': 72, '!': 73, 'n': 74, 'q': 75, 'z': 76, 'B': 77, 'w': 78, '+': 79, 'P': 80, 'Z': 81, 'R': 82, '_': 83, '.': 84, 'V': 85, '”': 86, '&': 87, '9': 88, 'L': 89}\n",
            "Example processed tweet: \n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18 50 38\n",
            " 59 67 32 50 38 59 67 32 50 38 59 67 32 50 78 59 22  1 61 74 48 50 78 61\n",
            " 62  7 50 48 22 32 13 62 50 52 32 59 52 38 32 84 52 61 29 84 62 78 61 62\n",
            " 62 32 22 84 29 59 54 26  6 27 19 57 54 64 85 38]\n",
            "Shape of tweet library:  (181, 280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1vvyci4qc_aN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "1fb42fa2-f20d-4c24-cb74-4c89f165c5bd"
      },
      "cell_type": "code",
      "source": [
        "al_tweets2, al_chars = tweet_preprocessing(al_tweets)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters :  104\n",
            "Size of tweet library (char) :  94043\n",
            "All characters: \n",
            " ['т', 'й', 'X', 'T', 'k', ';', 'D', ')', 'f', 'h', '-', 'J', '…', 'W', 'F', 'a', '5', '–', '\\n', '8', '1', 'í', 'I', 'K', 's', 'b', 'д', 'y', 'r', 'Q', 'A', '/', '‘', 'C', '’', 'р', 'c', 'Y', '?', 'в', 'e', ':', '(', 'u', '$', 'S', 'l', \"'\", '0', '—', 'x', 'd', 'j', '“', '\"', 'M', '7', 'g', 'с', 'а', 'E', ' ', '3', 'p', 'H', 'm', 'З', '™', '*', '2', 'é', 'O', '\\xa0', 'o', ',', 'i', 't', 'у', '@', '6', '4', 'v', 'N', 'G', '#', 'U', '%', '!', 'n', 'q', 'z', 'B', 'w', 'P', 'Z', 'R', '_', '.', 'V', '”', '&', 'е', '9', 'L']\n",
            "Character dictionary: \n",
            " {'т': 0, 'й': 1, 'X': 2, 'T': 3, 'k': 4, ';': 5, 'D': 6, ')': 7, 'f': 8, 'h': 9, '-': 10, 'J': 11, '…': 12, 'W': 13, 'F': 14, 'a': 15, '5': 16, '–': 17, '\\n': 18, '8': 19, '1': 20, 'í': 21, 'I': 22, 'K': 23, 's': 24, 'b': 25, 'д': 26, 'y': 27, 'r': 28, 'Q': 29, 'A': 30, '/': 31, '‘': 32, 'C': 33, '’': 34, 'р': 35, 'c': 36, 'Y': 37, '?': 38, 'в': 39, 'e': 40, ':': 41, '(': 42, 'u': 43, '$': 44, 'S': 45, 'l': 46, \"'\": 47, '0': 48, '—': 49, 'x': 50, 'd': 51, 'j': 52, '“': 53, '\"': 54, 'M': 55, '7': 56, 'g': 57, 'с': 58, 'а': 59, 'E': 60, ' ': 61, '3': 62, 'p': 63, 'H': 64, 'm': 65, 'З': 66, '™': 67, '*': 68, '2': 69, 'é': 70, 'O': 71, '\\xa0': 72, 'o': 73, ',': 74, 'i': 75, 't': 76, 'у': 77, '@': 78, '6': 79, '4': 80, 'v': 81, 'N': 82, 'G': 83, '#': 84, 'U': 85, '%': 86, '!': 87, 'n': 88, 'q': 89, 'z': 90, 'B': 91, 'w': 92, 'P': 93, 'Z': 94, 'R': 95, '_': 96, '.': 97, 'V': 98, '”': 99, '&': 100, 'е': 101, '9': 102, 'L': 103}\n",
            "Example processed tweet: \n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  95   3  61  78  83  40  73   8\n",
            "   8   3   9  40  95  73  25  73  76  41  61  64  40  27  61  30  46  74\n",
            "  61  27  73  43  61  63  46  15  27  40  51  61  90  27  51  40  36  73\n",
            "  61  73  88  61  65  27  61  28  75  25  24  61  15  76  61  76   9  40\n",
            "  61  95  60   6  61  63  28  40  65  75  40  28  40  61  15  88  51  61\n",
            "  75  76  61  15  75  28  24  61  76  73  88  75  57   9  76  61  73  88\n",
            "  61 103  15  76  40  61 103  15  76  40  61  92  75  76   9  61  78  33\n",
            "  28  15  75  57  27  14  40  28  57  87]\n",
            "Shape of tweet library:  (960, 280)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a9eOH3Yvjp1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2954eb4-ae75-407b-da59-b3da9c5f1ce2"
      },
      "cell_type": "code",
      "source": [
        "# Create a single vocabulary for both tweet libraries\n",
        "vocabulary = set(austen_chars + al_chars)\n",
        "len(vocabulary)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "5keV3aOHfm57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a829df77-2f74-4506-8a1f-94d117fe07e5"
      },
      "cell_type": "code",
      "source": [
        "# Combine both user's processed tweets into a single dataset\n",
        "X = np.concatenate((austen_tweets2, al_tweets2), axis=0)\n",
        "y = np.concatenate((np.zeros((austen_tweets2.shape[0],1)),\n",
        "                    np.ones((al_tweets2.shape[0],1))), axis=0)\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1141, 280), (1141, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "9ACqR_VOhgZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Divide into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                  test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i18UbV4Wib60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "d7a8e12b-03d2-4daa-d661-7dd05a9d4cf4"
      },
      "cell_type": "code",
      "source": [
        "# Create the Keras LSTM RNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(vocabulary), output_dim=128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, None, 128)         13568     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 145,281\n",
            "Trainable params: 145,281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mtFqMgt6h7Mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "2c222b6d-f9ee-4c54-e21b-6f58989fefda"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 855 samples, validate on 286 samples\n",
            "Epoch 1/15\n",
            "855/855 [==============================] - 20s 24ms/step - loss: 0.3684 - acc: 0.8901 - val_loss: 0.0765 - val_acc: 0.9615\n",
            "Epoch 2/15\n",
            "855/855 [==============================] - 18s 22ms/step - loss: 0.0244 - acc: 0.9953 - val_loss: 0.0324 - val_acc: 0.9895\n",
            "Epoch 3/15\n",
            "855/855 [==============================] - 18s 21ms/step - loss: 0.0169 - acc: 0.9965 - val_loss: 0.0145 - val_acc: 0.9965\n",
            "Epoch 4/15\n",
            "855/855 [==============================] - 18s 21ms/step - loss: 0.0095 - acc: 0.9977 - val_loss: 0.0133 - val_acc: 0.9965\n",
            "Epoch 5/15\n",
            "855/855 [==============================] - 18s 21ms/step - loss: 0.0090 - acc: 0.9977 - val_loss: 0.0123 - val_acc: 0.9965\n",
            "Epoch 6/15\n",
            "855/855 [==============================] - 18s 21ms/step - loss: 0.0048 - acc: 0.9988 - val_loss: 0.0106 - val_acc: 0.9965\n",
            "Epoch 7/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 0.0045 - acc: 0.9988 - val_loss: 0.0082 - val_acc: 0.9965\n",
            "Epoch 8/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9965\n",
            "Epoch 9/15\n",
            "855/855 [==============================] - 18s 22ms/step - loss: 0.0033 - acc: 0.9977 - val_loss: 0.0188 - val_acc: 0.9930\n",
            "Epoch 10/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0070 - val_acc: 0.9965\n",
            "Epoch 11/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9965\n",
            "Epoch 12/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 9.7292e-04 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9965\n",
            "Epoch 13/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 5.0728e-04 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9965\n",
            "Epoch 14/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 3.8604e-04 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9965\n",
            "Epoch 15/15\n",
            "855/855 [==============================] - 19s 22ms/step - loss: 3.4149e-04 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94f7e36208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "WZdJmSpmnADT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3f94b658-692e-4f83-a4f1-9b9268af9317"
      },
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print(f'Test accuracy: {acc*100:.2f}%')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "286/286 [==============================] - 1s 4ms/step\n",
            "Test score: 0.008031357738752751\n",
            "Test accuracy: 99.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zsVwdgS9q8bn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Whoa!  The model was 99.65% accurate in classifying the tweets!"
      ]
    },
    {
      "metadata": {
        "id": "O55z4qofrDe9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}